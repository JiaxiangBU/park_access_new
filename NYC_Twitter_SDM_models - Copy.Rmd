---
title: "NYC Spatial Models - Comparison of Spatial Models With and Without Twitter Data"
author: "Nico Boyd"
date: "August 10, 2017"
output: html_document
---

This document contains the model estimation and impacts results for the paper.


```{r setup, echo=FALSE, message=FALSE, warning=FALSE}
library(geojsonio)
library(tidyverse)
library(maptools)
library(rgdal)
library(rgeos)
library(spdep)
library(texreg)
library(leaflet)
library(sf)
library(VGAM)
```

Nico cleaned the tracts dataset into a geoJSON file stored in the repository. We
should read it in twice because the sf library is better for visualization but
spdep still requires sp objects for now.

```{r read_tracts}
tracts <- geojson_read("data/nyc_tracts.geojson", what = "sp")
tracts_sf <- st_read("data/nyc_tracts.geojson", quiet = TRUE) %>%
  st_transform(4326)
```


## Complete Cases
We noticed that our models had different numbers of observations / degrees of 
freedom. Obviously we need to have comparable econometric models. The first thing
we want to look for is missingness in the tracts variables, and which tracts
are missing those variables. The map below shows the tracts that are missing
this information.

```{r tracts_with_missing}
# identify complete cases
complete_tracts_index <- tracts_sf %>%
  st_set_geometry(NULL) %>%
  tbl_df() %>%
  complete.cases()

# make a map of 
leaflet( tracts_sf[!complete_tracts_index, ]) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>% 
  addPolygons(label = ~ as.character(GEOID))
```

These tracts seem to fall into two basic categories:

  - Tracts outside of New York City that were clipped into the shape layer 
  and remain as edge remnants. It is generally better to select by FIPS code rather
  than geometric operations, but we can just discard these at this point and
  they will not affect anything.
  
  - Tracts that are completely occupied by parks, airports, or other pieces
  of non-residential infrastructure. 
  
The exceptions to this appear to be 3606100940 and 36061010200 in Midtown between
5th Avenue and Park Avenue. Greg will assume for this analysis that these are 
entirely commercial tracts and therefore have no residents, but Nico needs to 
verify these.

```{r complete_cases}
tracts <- tracts[complete_tracts_index, ]
```

The parks data seems relatively complete, with the exception of a few variables
that are missing everywhere. But we'll ignore this for now.

```{r read_parks, echo=FALSE}
parks <- geojson_read("data/nyc_parks.geojson", what = "sp")
parks_sf <- st_read("data/nyc_parks.geojson") %>%
  st_transform(4326)
#plot(parks)
```

The Tweets count variable has a high degree of zeros (virtually all parks have
no tweets originating there). We should identify if there is a transformation
that will make the distribution more normal (log is typical), but that will allow
for zero variables as $log(0)$ is undefined. Using a Yeo-Johnson transformation
with a lambda value of zero transforms the tweet count into a distribution
asymptotically equivalent to log, and with zero values equal to zero.

```{r tweettransformation}
parks_sf %>%
  st_set_geometry(NULL) %>%
  tbl_df() %>%
  select(raw = TWEET_COUNT) %>%
  mutate(
    `0.5` = yeo.johnson(raw, 0.5),
    `0` = yeo.johnson(raw, 0),
    log = log(raw)
  ) %>%
  sample_n(200) %>%
  gather(lambda, x, -raw) %>%
  ggplot(aes(x = raw, y = x, color = lambda)) + 
  geom_line()
```



```{r show_data, echo=FALSE}
tracts@data <- tbl_df(tracts@data) %>%
  select(
    GEOID, OBESITY, Park_Percent, Phys_Act, MENTAL, 
    Income1, Income2, Income3, Income4, Income5, Income6, 
    Income7, Income8, Income9, Income10, Pop_Density, 
    FulltimeWork, CollegeDeg, Pct0to17, Pct18to29, Pct30to64, Pct65plus, 
    Single_Percent, PctWhite, PctBlack, PctNative, PctAsian, PctPacific,
    PctOther, PctHispanic
  ) 

parks@data <- tbl_df(parks@data) 

tracts@data
parks@data
```


Instead of keeping those functions in separate scripts, we are going to put
everything right here in the document.


```{r logsumfunction}

#' Calculate destination choice logsums from a distance matrix
#' 
#' @param tracts A spatial points dataframe containing $n$
#'   tract geographic information and their attributes.
#' @param parks A spatial points dataframe containing $p$
#'   park geographic information and their attributes.
#' @param distances An $n\times p$ matrix with the distance from all tracts to
#'   all parks in miles.
#' @param sizes A p-length vector of park sizes
#' @param tweets A p-length vector of tweets at parks
#' @param betas A vector containing the size, distance, and tweet coefficients. 
#'   If we submit two variables the tweets are ignored.
#'   
#' @return An n-length vector containing the weighted log-sum based
#'   accessibility between a tract and all parks.
#' @details If we have n tracts and p parks, distances needs to be a 
#' 
calculate_park_logsums <- function(tracts, parks, distances, sizes, tweets = NULL,
                                   betas = c(.00001, -15, .001)){
  
  # A is n x p
  a <- betas[2] * distances 
  
  # B is p x 1
  b <- betas[1] * sizes
  
  if(!is.null(tweets)) 
    b <- b + betas[3] * tweets
  
  # calculate observed utility by adding the weighted park-level attributes 
  # to the columns of the matrix
  # V is n x p, with b added by-column to each element in a
  V <- sweep(a, MARGIN = 2, b, `+`)
  
  # log-sum of exponentiated utility, Output is n-length vector
  log(rowSums(exp(V)))
  
}
```


```{r distancematrix}
# calculate centroids of tracts and parks
tract_centroids <- gCentroid(tracts, byid = TRUE) 
park_centroids <- gCentroid(parks, byid = TRUE)

# distances from all tracts to all zones
distances <- gDistance(park_centroids, tract_centroids, byid = TRUE) / 5280
```

Calculate the logsum given a tracts and parks dataset. `tracts$park_ls1` uses
only size and distance parameters. `tracts$park_ls2` uses size, distance and
tweet parameters. `tracts$park_ls4` and `tracts$park_ls5` use a distance
parameter of -15.

We need to firm up these calculations. Are we using log of acres as well?

```{r logsums}
tracts$park_ls1 <- calculate_park_logsums(tracts, parks, distances,
                                          parks$Park_Acres, betas = c(.00001, -5))

tracts$park_ls2 <- calculate_park_logsums(tracts, parks, distances,
                                          parks$Park_Acres,
                                          yeo.johnson(parks$TWEET_COUNT, 0),
                                          betas = c(.00001, -5, 2))

tracts$park_ls4 <- calculate_park_logsums(tracts, parks, distances,
                                          parks$Park_Acres, betas = c(.00001, -15))

tracts$park_ls5 <- calculate_park_logsums(tracts, parks, distances,
                                          parks$Park_Acres,
                                          yeo.johnson(parks$TWEET_COUNT, 0),
                                          betas = c(.00001, -15, 2))
ls_names <- c("Short, no tweets",  "Short, tweets",  
              "Long, no tweets",  "Long, tweets")
```


I'm interested in looking at the distribution of the log-sum statistics so we can
know what if anything the different variables are doing, also to make sure that
there is no missingness in the data. 

  - `ls1` and `ls2` have virtually the same distribution, along with `ls4` and
  `ls5`. This suggests that tweets have virtually no effect on the rankings, at
  least in this iteration.

```{r distribution}
logsums <- tracts@data %>%
  select(park_ls1:park_ls5) %>%
  gather(logsum_type, value)

# check missingness
table(is.na(logsums$value))

# histogram
ggplot(logsums, aes(x = value, fill = logsum_type)) + geom_histogram()
```


Map the newly created Park Access variables. Here is a map of accessibility using only the size and distance parameters that are most likely when using obesity as the dependent variable.
```{r accessibility_leaflet_1, echo=FALSE}
# Create a continuous palette function
pal1 <- colorQuantile(palette = "Greens", domain = tracts$park_ls1)
pal2 <- colorQuantile(palette = "Greens", domain = tracts$park_ls2)
pal4 <- colorQuantile(palette = "Greens", domain = tracts$park_ls4)
pal5 <- colorQuantile(palette = "Greens", domain = tracts$park_ls5)


leaflet(tracts %>% spTransform(., CRS("+init=epsg:4326"))) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(group = ls_names[1], weight = 1, color = ~pal1(park_ls1)) %>%
  addPolygons(group = ls_names[2], weight = 1, color = ~pal2(park_ls2)) %>%
  addPolygons(group = ls_names[3], weight = 1, color = ~pal4(park_ls4)) %>%
  addPolygons(group = ls_names[4], weight = 1, color = ~pal5(park_ls5)) %>%
  addLayersControl(baseGroups = ls_names)
```




# Obesity model estimation

Create a spatial weights matrix. It's showing that we have three regions with
no links, so we need to ensure that these are estimated in the model.

```{r W, echo=FALSE}
neighbors <- poly2nb(tracts, queen = TRUE)
W <- nb2listw(neighbours = neighbors, zero.policy = TRUE)
print(W, zero.policy = TRUE)
trMC <- trW(as(W, "CsparseMatrix"), type="MC") # trace used in montecarlo impacts
```

```{r obese_lm, echo=FALSE, message=FALSE, include=FALSE}
base_formula <- formula( 
  ~ log(Pop_Density) + 
    FulltimeWork + CollegeDeg + Single_Percent + 
    Pct0to17 + Pct18to29 + Pct65plus + # need to leave out a category for collinearity  
    PctBlack + PctAsian + PctOther + PctHispanic)

parks_formula_1 <- update(base_formula, . ~ . + park_ls1 )
parks_formula_2 <- update(base_formula, . ~ . + park_ls2 )
parks_formula_4 <- update(base_formula, . ~ . + park_ls4 )
parks_formula_5 <- update(base_formula, . ~ . + park_ls5 )
```

First, let's estimate the base model as an LM, SDM, and SEM model

```{r basemodels}
# Time consuming.
obese_base_lm <- lm(update(base_formula, OBESITY ~ .), data = tracts@data)
obese_base_sar <- lagsarlm(update(base_formula, OBESITY ~ .), 
                           data = tracts@data, listw = W, zero.policy = TRUE)
obese_base_sem <- errorsarlm(update(base_formula, OBESITY ~ .), 
                             data = tracts@data, listw = W, zero.policy = TRUE)
obese_base_sdm <- lagsarlm(update(base_formula, OBESITY ~ .), 
                           data = tracts@data, listw = W, zero.policy = TRUE, 
                           type = "mixed")
```

A likelihood ratio test reveals that the SEM is not preferred, so we use the SDM
only going forward.

```{r lrtest}
lmtest::lrtest(obese_base_sem, obese_base_sdm)
```


```{r obesity, echo=FALSE, message=FALSE, warning=FALSE}
# Time consuming.
obese_ls1_sdm <- lagsarlm(update(parks_formula_1, OBESITY ~ .), 
                          data = tracts@data,  listw = W, zero.policy = TRUE, type = "mixed")
obese_ls2_sdm <- lagsarlm(update(parks_formula_2, OBESITY ~ .), 
                          data = tracts@data,  listw = W, zero.policy = TRUE, type = "mixed")
obese_ls4_sdm <- lagsarlm(update(parks_formula_4, OBESITY ~ .), 
                          data = tracts@data,  listw = W, zero.policy = TRUE, type = "mixed")
obese_ls5_sdm <- lagsarlm(update(parks_formula_5, OBESITY ~ .), 
                          data = tracts@data,  listw = W, zero.policy = TRUE, type = "mixed")

obesity_models <- list(obese_base_sdm, obese_ls1_sdm, obese_ls2_sdm,
                       obese_ls4_sdm, obese_ls5_sdm)
names(obesity_models) <- c("Base", ls_names)
```

Estimate spatial models for the `Phys_Act` variable. Model 1 does not include park access. Models 2 & 3 each use a different park access variable with a different combination of parameters:

```{r}
screenreg(obesity_models)
```


Estimate the impacts for `obese_sdm_age`.

```{r obese_sdm_age_impacts}



obesity_models %>%
  lapply(impacts_extractor) %>%
  bind_rows(.id = "model") %>%
  transmute(
    model, var, effect, 
    output = paste(round(impact, 5), 
                   gtools::stars.pval(`p-val`))
  ) %>%
  spread(model, output)

```

<!-- # Estimate Physical Activity Models -->

```{r phy, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
physact_base_sdm <- lagsarlm(update(base_formula, Phys_Act ~ .), 
                             data = tracts@data,  listw = W, zero.policy = TRUE, 
                             type = "mixed")
physact_ls1_sdm <- lagsarlm(update(parks_formula_1, Phys_Act ~ .), 
                            data = tracts@data,  listw = W, zero.policy = TRUE, 
                            type = "mixed")
physact_ls2_sdm <- lagsarlm(update(parks_formula_2, Phys_Act ~ .), 
                            data = tracts@data,  listw = W, zero.policy = TRUE, 
                            type = "mixed")
physact_ls4_sdm <- lagsarlm(update(parks_formula_4, Phys_Act ~ .), 
                            data = tracts@data,  listw = W, zero.policy = TRUE, 
                            type = "mixed")
physact_ls5_sdm <- lagsarlm(update(parks_formula_5, Phys_Act ~ .), 
                            data = tracts@data,  listw = W, zero.policy = TRUE, 
                            type = "mixed")

physact_models <- list(physact_base_sdm, physact_ls1_sdm, physact_ls2_sdm,
                       physact_ls4_sdm, physact_ls5_sdm)
names(physact_models) <- c("Base", ls_names)
```

```{r physact_impacts, eval=FALSE, include=FALSE}
physact_models %>%
  lapply(impacts_extractor) %>%
  bind_rows(.id = "model") %>%
  transmute(
    model, var, effect, 
    output = paste(round(impact, 5), 
                   gtools::stars.pval(`p-val`))
  ) %>%
  spread(model, output) %>% View
```
