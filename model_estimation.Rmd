---
title: "Model Estimation"
output:
  html_document:
    df_print: paged
editor_options: 
  chunk_output_type: console
---
```{r setup}
library(geojsonio)
library(tidyverse)
library(maptools)
library(rgdal)
library(rgeos)
library(spdep)
library(texreg)
library(leaflet)
library(sf)
library(VGAM)
```


# Data cleaning

Nico cleaned the tracts dataset into a geoJSON file stored in the repository. We
should read it in twice because the sf library is better for visualization but
spdep still requires sp objects for now.


## Complete Cases
We noticed that our models had different numbers of observations / degrees of 
freedom. Obviously we need to have comparable econometric models. The first thing
we want to look for is missingness in the tracts variables, and which tracts
are missing those variables. The map below shows the tracts that are missing
this information.
```{r read_tracts}
tracts <- geojson_read("data/nyc_tracts.geojson", what = "sp")
tracts_sf <- st_read("data/nyc_tracts.geojson")

# identify complete cases
complete_tracts_index <- tracts@data %>%
  tbl_df() %>%
  complete.cases()
tracts <- tracts[complete_tracts_index, ]


tracts@data <-  tbl_df(tracts@data) %>%
  dplyr::transmute(
    geoid = GEOID,
    county_fips = substr(GEOID, 3, 5),
    borough = case_when(
      county_fips == "081" ~ "Queens",
      county_fips == "047" ~ "Brooklyn",
      county_fips == "061" ~ "Manhattan",
      county_fips == "005" ~ "Bronx",
      county_fips == "085" ~ "Staten Island",
      TRUE ~ as.character(NA)
    ),
    obesity = OBESITY,
    physact = 100 - Phys_Act,
    log_obesity = log(OBESITY),
    log_physact = log(100 - Phys_Act),
    density = Pop_Density,
    fulltime = FulltimeWork, college = CollegeDeg,
    single = Single_Percent,
    black = PctBlack, asian = PctAsian, hispanic = PctHispanic,
    other = PctNative + PctPacific,
    Pct0to17, Pct18to29, Pct65plus,
    income = NYC_Income
  )
```


```{r tracts_with_missing}
# make a map of 
leaflet( tracts_sf[!complete_tracts_index, ] %>% st_transform(4326)) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>% 
  addPolygons(label = ~ as.character(GEOID))
```

These tracts seem to fall into two basic categories:

  - Tracts outside of New York City that were clipped into the shape layer 
  and remain as edge remnants. It is generally better to select by FIPS code rather
  than geometric operations, but we can just discard these at this point and
  they will not affect anything.
  
  - Tracts that are completely occupied by parks, airports, or other pieces
  of non-residential infrastructure. 
  
The exceptions to this appear to be 3606100940 and 36061010200 in Midtown between
5th Avenue and Park Avenue. Greg will assume for this analysis that these are 
entirely commercial tracts and therefore have no residents, but Nico needs to 
verify these.


The parks data seems relatively complete, with the exception of a few variables
that are missing everywhere. Also it seems that we are including parking strips 
very small pieces of property, even though these facilities are unlikely to 
generate physical activity - related trips. Finally, sports fields within larger
parks probably do not need to be double-counted.

```{r read_parks, echo=FALSE}
allparks <- sf::st_read("data/nyc_parks.geojson", quiet = TRUE) %>%
  transmute(
    size = Park_Acres,
    log_size = log(Park_Acres),
    tweets = TWEET_COUNT,
    log_tweets = yeo.johnson(TWEET_COUNT, 0)
  )

parks <- allparks %>%
  filter(size > 0.5)
```

The map below shows the parks we are including.
```{r parksmap}
leaflet( allparks %>% filter(size > 0.5) %>% st_transform(4326)) %>%
  addProviderTiles(providers$OpenStreetMap.Mapnik) %>% 
  addPolygons()
```


## Tweets Transformation

The Tweets count variable has a high degree of zeros (virtually all parks have
no tweets originating there). We should identify if there is a transformation
that will make the distribution more normal (log is typical), but that will allow
for zero variables as $log(0)$ is undefined. Using a Yeo-Johnson transformation
with a lambda value of zero transforms the tweet count into a distribution
asymptotically equivalent to log, and with zero values equal to zero.

```{r tweettransformation}
data_frame(x = seq(0, 50, by = 0.5)) %>% 
  mutate(log = log(x), `yeo-johnson` = yeo.johnson(x, 0)) %>%
  gather(transform, y, -x) %>%
  ggplot(aes(x = x, y = y, color= transform)) + geom_line()
  
```

## Other variables

```{r show_data, echo=FALSE}
tracts@data
```

# Spatial Model Selection

Deciding which spatial model to select is an empirical question as outlined by
both LeSage and Pace (2010?) and Macfarlane et al. (2015) We estimate a model
without park accessibility to determine which spatial structure is correct
for our analysis.

We assert a first-order queen adjacency neighbors matrix, which is
row-standardized such that the total weight of each row in the matrix sums to 1.
There are three tracts in the data that have no adjacent neighbors, meaning that 
these tracts will have no spatial spillover effects.

```{r W, echo=FALSE}
tracts.dnn <- dnearneigh(gCentroid(tracts, byid = TRUE), 0, 1.8 * 5280)
dists <- nbdists(tracts.dnn, gCentroid(tracts, byid = TRUE))
dists.inv <- lapply(dists, function(x) 1 / x)
W <- nb2listw(neighbours = tracts.dnn, glist = dists.inv, zero.policy = TRUE, style = "W")
print(W, zero.policy = TRUE)
trMC <- trW(as(W, "CsparseMatrix"), type="MC") # trace used in montecarlo impacts
```

The base model regresses the obesity rate in the tract against covariates for
work status and educational attainment, minority and young/elderly population, 
and marital status.

```{r obese_lm, echo=FALSE, message=FALSE, include=FALSE}
base_formula <- formula(
  ~ log(density) + log(income) +
    fulltime + college + single +
    Pct0to17 + Pct18to29 + Pct65plus + # need to leave out a category for collinearity
    black + asian + hispanic + other)
```

We estimate four models with this regression specification:

  - Ordinary least squares
  - Spatial autoregressive
  - Spatial error
  - Spatial Durbin
  
  
```{r spatial selection}
physact_base_lm <- lm(update(base_formula, physact ~ .), data = tracts)
physact_base_sar <- lagsarlm(update(base_formula, physact ~ .),
                           data = tracts@data, listw = W, zero.policy = TRUE)
physact_base_sem <- errorsarlm(update(base_formula, physact ~ .),
                             data = tracts@data, listw = W, zero.policy = TRUE)
physact_base_sdm <- lagsarlm(update(base_formula, physact ~ .),
                           data = tracts@data, listw = W, zero.policy = TRUE,
                           type = "mixed")
test_sdmsem <- lmtest::lrtest(physact_base_sem, physact_base_sdm)
```


A likelihood ratio test reveals that the SEM actually preferred, so we use this
only going forward.

```{r }
physact_models <- list("OLS" = physact_base_lm, "SAR" = physact_base_sar, 
                       "SEM" = physact_base_sem, "SDM" = physact_base_sdm)
screenreg(physact_models, digits = 5)

```

The residuals appear homoskedastic with few outliers.

```{r residuals}
tracts_sf <- tracts_sf %>%
  mutate(
    log_obesity = tracts$log_physact,  
    physact_base_lm_residuals = physact_base_lm$residuals,
    physact_base_sem_fitted = physact_base_sem$fitted.values,  
    physact_base_sem_residuals = physact_base_sem$residual
  ) 

ggplot(tracts_sf, aes(x = log_obesity, y = physact_base_sem_fitted)) + 
  geom_point()
```

The residuals also appear to be distributed wtihout serious spatial correlation
```{r residuals_map}
pal <- colorQuantile("BrBG", tracts_sf$physact_base_sem_residuals, n = 9)
leaflet(tracts_sf %>% st_transform(4326)) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(color = ~pal(physact_base_sem_residuals), stroke = NA, opacity = 0.8)
```



# Logsum Calculation


$$ A_i = log\sum_{j \in J}\exp(distance_{ij} \beta_d + size_j \beta_j + tweets_j \beta_t)$$
We wrote a function to calculate this value for an arbitrary distance matrix, size
and tweets vector, and coefficients.

```{r logsums_function}

#' Calculate destination choice logsums from a distance matrix
#' 
#' @param d An $n\times p$ matrix with the distance from all tracts to
#'   all parks
#' @param sizes A p-length vector of park sizes
#' @param tweets A p-length vector of tweets at parks
#' @param betas A vector containing the size, distance, and tweet coefficients. 
#'   If we submit two variables the tweets are ignored.
#'   
#' @return An n-length vector containing the weighted log-sum based
#'   accessibility between a tract and all parks.
#' @details If we have n tracts and p parks, distances needs to be a 
#' 
calculate_park_logsums <- function(d, sizes, tweets = NULL,
                                   betas = c(-15,.00001, .001)){
  
  # A is n x p
  a <- betas[1] * d
  
  # B is p x 1
  b <- betas[2] * sizes
  
  if(!is.null(tweets)) 
    b <- b + betas[3] * tweets
  
  # calculate observed utility by adding the weighted park-level attributes 
  # to the columns of the matrix
  # V is n x p, with b added by-column to each element in a
  V <- sweep(a, 2, b, `+`)
  
  # log-sum of exponentiated utility, Output is n-length vector
  log(rowSums(exp(V)))
  
}
```



First, we calculate the distance between the tracts and the parks.
```{r distancematrix}
# calculate centroids of tracts and parks
parks <- parks %>% st_transform(st_crs(pop_weighted_centroids))

# distances from all tracts to all zones
distances <- st_distance(pop_weighted_centroids, parks, byid = TRUE) %>%
  units::set_units(miles) %>%
  units::drop_units()

# assert that a tract must be at least 1/10 mile from a park
distances <- pmax(distances, 0.1)
```


We now need to run a subroutine to determine the values of the beta coefficients
which produce the highest model likelihood, and .


```{r notweets_logsum, eval = FALSE}
notweets_logsum <- function(betas, distances, parks, tracts, model) {
  
  logsum <- calculate_park_logsums(
    d = distances,  sizes = parks$log_size,
    betas = betas)
  
  tracts@data$access_ls <- (logsum - mean(logsum)) / sd(logsum)
  
  sdm <- update(model, . ~ . + access_ls)
  return(-logLik(sdm))
}

access_sdm_val <- optim(
  fn = notweets_logsum,  
  par = c(-0.42, 0.5), # distance, size 
  lower = c(-10, 0), # size coefficient must be positive 
  upper = c(0, 10), # distance coefficient must be negative 
  distances = log(distances), parks = parks, tracts = tracts, 
  model = physact_base_sem,
  method = "L-BFGS-B", 
  control = list(trace = 6))
```




```{r tweets_logsum, eval = FALSE}
tweets_logsum <- function(betas, distances, parks, tracts, model) {
  
  logsum <- calculate_park_logsums(
    d = distances,  sizes = parks$log_size, tweets = parks$log_tweets,
    betas = betas)
  
  tracts@data$tweets_ls <- (logsum - mean(logsum)) / sd(logsum)
  
  sdm <- update(model, . ~ . + tweets_ls)
  return(-logLik(sdm))
}

tweets_sdm_val <- optim(
  fn = tweets_logsum,  
  par = c(-0.42, 0.5, 1), # distance, size 
  lower = c(-10, 0, -10), # size coefficient must be positive 
  upper = c(0, 10, 10), # distance coefficient must be negative 
  distances = log(distances), parks = parks, tracts = tracts, 
  model = physact_base_sem,
  method = "L-BFGS-B", 
  control = list(trace = 6))

```



```{r show_accessibility}
tracts@data <- tracts@data %>%
  mutate(
    access_ls = calculate_park_logsums(log(distances), parks$log_size,
                                       betas = c(-1.796, 1.0)),
    tweets_ls = calculate_park_logsums(log(distances), parks$log_size, parks$log_tweets,
                                       betas = c(-1.796, 1.0, 0.1)),
    access_ls = (access_ls - mean(access_ls) )/ sd(access_ls),
    tweets_ls = (tweets_ls - mean(tweets_ls) )/ sd(tweets_ls)
  )

tracts_sf <- tracts_sf %>%
  mutate(
    access_ls = tracts$access_ls,
    tweets_ls = tracts$tweets_ls
  )

pal <- colorQuantile("Spectral", c(tracts_sf$access_ls, tracts_sf$tweets_ls), 5)

leaflet(tracts_sf %>% st_transform(4326)) %>%
  addProviderTiles(providers$CartoDB.Positron) %>%
  addPolygons(group = "access", color = ~pal(access_ls)) %>%
  addPolygons(group = "access + tweets", color = ~pal(tweets_ls)) %>%
  addLayersControl(baseGroups = c("access", "access + tweets"))

```

```{r obesity_access_models}
access_sem <- update(physact_base_sem, .~ . + access_ls)
tweets_sem <- update(physact_base_sem, .~ . + tweets_ls)
```



```{r modelchart}
physact_models <-  
screenreg()

```



```{r impacts_extractor}
#' Get the MC simulated impact coefficients and significance from a model
#' @param sdm An autoregressive lag (SAR or SDM) model object `lagsarlm`
#' @return A data frame with the effect (direct, indirect, total), variable, 
#'   simulated mean impact, and significance p-value
impacts_extractor <- function(sdm) {
  
  impacts_summary <- summary(
    impacts(sdm, tr=trMC,  R = 1000, useHESS = !is.logical(sdm$fdHess)), 
    zstats = TRUE)
  
  coef <- list(
    Direct = impacts_summary$direct_sum,
    Indirect = impacts_summary$indirect_sum,
    Total = impacts_summary$total_sum
  ) %>%
  lapply(function(s) {
    data_frame(
      var = names(s$statistics[,1]),
      impact = s$statistics[,1]
    )
  }) %>%
    bind_rows(.id = "effect")
  
  pval <- as_data_frame(impacts_summary$pzmat) %>%
    mutate(
     var = rownames(impacts_summary$pzmat)
    ) %>%
    gather("effect", "p-val", -var)
  
  
  left_join(coef, pval, by = c("effect", "var"))
}
```


```{r obesity_impacts}
obesity_impacts <- obesity_models %>%
  lapply(impacts_extractor) %>%
  bind_rows(.id = "model") %>%
  transmute(
    model, var, effect, 
    output = paste(round(impact, 5), 
                   gtools::stars.pval(`p-val`))
  ) %>%
  spread(model, output) 
```



# Physical Activity

We cannot necessarily assume that the logsum parameter will be constructed the
same way for physical activity as for obesity. (maybe?) But that's the assumption
we'll make for now.

```{r physact_estimation}
pa_base_sdm <- update(obese_base_sdm, log_physact ~ .)
pa_access_sdm <- update(access_sdm, log_physact ~ .)
pa_tweets_sdm <- update(tweets_sdm, log_physact ~ .)
```

```{r physact_models}
physact_models <- list(Base = pa_base_sdm, Access = pa_access_sdm, 
                       `Access + Tweets` = pa_tweets_sdm)
screenreg(physact_models, digits = 4)
```


```{r physact_impacts}
physact_impacts <- physact_models %>%
  lapply(impacts_extractor) %>%
  bind_rows(.id = "model") %>%
  transmute(
    model, var, effect, 
    output = paste(round(impact, 5), 
                   gtools::stars.pval(`p-val`))
  ) %>%
  spread(model, output) 
```



# Results
```{r impacts_table}
list("Obesity" = obesity_impacts, "Physical Activity" = physact_impacts) %>%
  bind_rows(.id = "Outcome") %>%
  filter(var %in% c("access_ls", "tweets_ls")) %>%
  dplyr::select(-var, -Base)  %>%
  gather("Measure", "Impact", -Outcome, -effect) %>%
  filter(!is.na(Impact))  %>%
  spread(effect, Impact)

```



